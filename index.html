<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Style Aligned Artwork Datasets</title>
  <meta name="title" content="Style Aligned Artwork Datasets" />
  <meta name="description" content="Project page of the dataset paper 'fruit-SALAD: A Style Aligned Artwork Dataset to reveal similarity perception in image embeddings'" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://style-aligned-artwork-datasets.github.io/" />
  <meta property="og:title" content="Style Aligned Artwork Datasets" />
  <meta property="og:description" content="Project page of the dataset paper 'fruit-SALAD: A Style Aligned Artwork Dataset to reveal similarity perception in image embeddings'" />
  <meta property="og:image" content="https://style-aligned-artwork-datasets.github.io/static/images/SALAD_preview.png" />
  <meta property="twitter:card" content="summary_large_image" />
  <meta property="twitter:url" content="https://style-aligned-artwork-datasets.github.io/" />
  <meta property="twitter:title" content="Style Aligned Artwork Datasets" />
  <meta property="twitter:description" content="Project page of the dataset paper 'fruit-SALAD: A Style Aligned Artwork Dataset to reveal similarity perception in image embeddings'" />
  <meta property="twitter:image" content="https://style-aligned-artwork-datasets.github.io/static/images/SALAD_preview.png" />

  
  <!-- Google scholar tags-->
  <meta name="citation_title" content="fruit-SALAD: A Style Aligned Artwork Dataset to reveal similarity perception in image embeddings" />
  <meta name="citation_author" content="Tillmann Ohm" />
  <meta name="citation_author" content="Andres Karjus" />
  <meta name="citation_author" content="Mikhail Tamm" />
  <meta name="citation_author" content="Maximilian Schich" />
  <meta name="citation_journal_title" content="arXiv" />
  <meta name="citation_abstract" content="The notion of visual similarity is essential for computer vision, and in applications and studies revolving around vector embeddings of images. However, the scarcity of benchmark datasets poses a significant hurdle in exploring how these models perceive similarity. Here we introduce Style Aligned Artwork Datasets (SALADs), and an example of fruit-SALAD with 10,000 images of fruit depictions. This combined semantic category and style benchmark comprises 100 instances each of 10 easy-to-recognize fruit categories, across 10 easy distinguishable styles. Leveraging a systematic pipeline of generative image synthesis, this visually diverse yet balanced benchmark demonstrates salient differences in semantic category and style similarity weights across various computational models, including machine learning models, feature extraction algorithms, and complexity measures, as well as conceptual models for reference. This meticulously designed dataset offers a controlled and balanced platform for the comparative analysis of similarity perception. The SALAD framework allows the comparison of how these models perform semantic category and style recognition task to go beyond the level of anecdotal knowledge, making it robustly quantifiable and qualitatively interpretable."/>
  <meta name="citation_pdf_url" content="https://www.nature.com/articles/s41597-025-04529-4" />
  

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css"> 
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico" type="image/x-icon">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    ul {
      list-style-type: disc; /* This sets the default bullet style */
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">fruit-SALAD:</h1>
          <h2 class="is-size-3 publication-title">
            A Style Aligned Artwork Dataset<br>to reveal similarity perception in image embeddings
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tillmannohm.com/" target="_blank" rel="noopener noreferrer">Tillmann Ohm</a><sup>‡</sup><sup>,</sup><sup>1</sup><sup>,</sup><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="https://andreskarjus.github.io/" target="_blank" rel="noopener noreferrer">Andres Karjus</a><sup>2</sup><sup>,</sup><sup>3</sup><sup>,</sup><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=JETUo9AAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Mikhail Tamm</a><sup>1</sup><sup>,</sup><sup>4</sup><sup>,</sup><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.schich.info/" target="_blank" rel="noopener noreferrer">Maximilian Schich</a><sup>3</sup><sup>,</sup><sup>6</sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup><a href="https://www.tlu.ee/en/dt" target="_blank" rel="noopener noreferrer">School of Digital Technology, Tallinn University, Estonia</a></span>
            <span class="author-block"><sup>2</sup><a href="https://www.tlu.ee/en/ht" target="_blank" rel="noopener noreferrer">School of Humanities, Tallinn University, Estonia</a></span>
            <span class="author-block"><sup>3</sup><a href="https://www.ebs.ee/en" target="_blank" rel="noopener noreferrer">Estonian Business School, Estonia</a></span>
            <span class="author-block"><sup>4</sup><a href="https://www.complexsimplex.com/" target="_blank" rel="noopener noreferrer">ComplexSimplex Group, National Institute of Chemical Physics and Biophysics, Estonia</a></span>
            <span class="author-block"><sup>5</sup><a href="https://www.tlu.ee/en/bfm" target="_blank" rel="noopener noreferrer">Baltic Film, Media, and Arts School, Tallinn University, Estonia</a></span><br/>
            <span class="author-block"><sup>6</sup><a href="http://cudan.tlu.ee/" target="_blank" rel="noopener noreferrer">ERA Chair for Cultural Data Analytics, Tallinn University, Estonia</a></span><br/>
            <span class="author-block"><sup>‡</sup>Corresponding author: mail@tillmannohm.com</span><br/>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://style-aligned-artwork-datasets.github.io/fruit-explorer"
                target="_blank" rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-desktop"></i>
                  </span>
                  <span>Interactive</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.nature.com/articles/s41597-025-04529-4"
                target="_blank" rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-alt"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://zenodo.org/records/11158522"
                target="_blank" rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/Style-Aligned-Artwork-Datasets/fruit-SALAD"
                target="_blank" rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <img src="./static/images/Figure1.webp">
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The notion of visual similarity is essential for computer vision, and in applications and studies revolving around vector
            embeddings of images. However, the scarcity of benchmark datasets poses a significant hurdle in exploring how these
            models perceive similarity. Here we introduce Style Aligned Artwork Datasets (SALADs), and an example of fruit-SALAD with
            10,000 images of fruit depictions. This combined semantic category and style benchmark comprises 100 instances each of
            10 easy-to-recognize fruit categories, across 10 easy distinguishable styles. Leveraging a systematic pipeline of generative
            image synthesis, this visually diverse yet balanced benchmark demonstrates salient differences in semantic category and style
            similarity weights across various computational models, including machine learning models, feature extraction algorithms, and
            complexity measures, as well as conceptual models for reference. This meticulously designed dataset offers a controlled and
            balanced platform for the comparative analysis of similarity perception. The SALAD framework allows the comparison of how
            these models perform semantic category and style recognition task to go beyond the level of anecdotal knowledge, making it
            robustly quantifiable and qualitatively interpretable.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>


<!-- Dataset Construction. -->
<section class="section">
  <div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Dataset Construction</h2>

    <div class="content has-text-justified">
      The fruit-SALAD benchmark consists of 10,000 synthetic images representing:    
      <ul>
        <li>10 fruit categories (e.g., apple, banana, kiwi)</li>
        <li>10 artistic styles (e.g., crayon, watercolor, pixel art)</li>
        <li>100 images per fruit-style combination</li>
      </ul>
      <br>
      To generate the fruit-SALAD dataset, we developed a systematic pipeline leveraging <a href="https://github.com/Stability-AI/generative-models" target="_blank" rel="noopener noreferrer">Stable Diffusion</a> XL (SDXL) and <a href="https://github.com/google/style-aligned" target="_blank" rel="noopener noreferrer">StyleAligned</a>. We first experimented with various style prompts and fruit categories, selecting successful examples as reference images. Using StyleAligned with diffusion inversion, we then generated multiple instances of each fruit within the same style. After iterative refinements, we automated the process to produce 100 instances per fruit-style combination, ensuring both prototypicality and stylistic coherence. Throughout the supervised process, we manually filtered out inconsistent generations to maintain quality and coherence. As a result, the dataset inherently reflects our own biases in curating the final selection.
    </div>
    <!-- image -->
     <div class="columns is-centered">
      <div class="column is-full-width">
        <figure class="image">
          <img src="./static/images/SALAD-pipeline.webp" alt="Overview of the image generation process." class="image">
        </figure>
      </div>
    </div>
    <!-- image -->

</div>
</section>
<!--/ Dataset Construction. -->

<!-- Key Characteristics. -->
<section class="section">
  <div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Key Characteristics</h2>

    <div class="content has-text-justified">
      Existing benchmark datasets lack the precise control to evaluate whether models primarily focus on the semantic category of an image (e.g., an apple) or its stylistic representation (e.g., watercolor painting). The fruit-SALAD dataset addresses this gap by offering a structured way to analyze these dimensions separately, enabling a precise study of how models weigh these factors. By maintaining a balanced dataset with equal representation of each fruit-style combination, it provides an interpretable benchmark for assessing model behavior. This makes the fruit-SALAD a valuable tool for both quantitative analysis and human-interpretable insights in various research fields.
    </div>
  </div>
</div>
</section>
<!--/ Key Characteristics. -->

<!-- Comparison & Analysis. -->
<section class="section">
  <div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Comparison & Analysis</h2>

    <div class="content has-text-justified">
      To investigate how computational models process visual similarity, we constructed image embeddings by extracting features with a variety of common pre-trained deep learning models, including Vision Transformers, DINO, CLIP, ResNet, and others. These models differ in architecture and training paradigms, allowing for a diverse comparison of similarity perception. Additionally, we explored alternative representations, including our <a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00397-3" target="_blank" rel="noopener noreferrer">Compression Ensembles</a> and one-hot encoded vectors, as reference models (e.g. “style_blind”).
    </div>
  </div>
</div>
</section>
<!--/ Comparison & Analysis. -->

<!-- Self-Recognition Test. -->
<section class="section">
  <div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Self-Recognition Test</h2>

    <div class="content has-text-justified">
      We conducted a self-recognition test to assess whether models could correctly identify images of the same fruit-style combination. Similarity was measured by counting how many matching images appeared in the top 100 nearest neighbors.
      <br><br>
      Interestingly, certain fruit-style pairs – such as apples and oranges in watercolor – proved challenging across all models, while specific combinations posed difficulties for individual models. This highlights differences in how computational models perceive semantic content and artistic style.  
    </div>
        <!-- image -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <figure class="image">
              <img src="./static/images/Self-Recognition_Test.webp" alt="Self-recognition tests." class="image">
            </figure>
          </div>
        </div>
        <!-- image -->
  </div>
</div>
</section>
<!--/ Self-Recognition Test. -->



<!-- Model Heatmaps. -->
<section class="section">
  <div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Model Heatmaps</h2>

    <div class="content has-text-justified">
      To further visualize model behavior, we created “double-heatmaps” that illustrate their similarity perception within the dataset. Each matrix cell represents the average distance across 10,000 image pairs. To facilitate comparison, we sorted the matrix in two different ways: below the diagonal, images are arranged first by style and then by fruit category, while above the diagonal, the order is reversed – first by fruit category and then by style. 
      <br><br>
      The heatmaps use a color gradient from blue to yellow, where blue signifies lower distances (higher similarity) and yellow represents higher distances (lower similarity). This visualization helps to reveal patterns in how models encode and differentiate the images. For instance, strong blue clusters along one axis may indicate a model’s bias toward grouping images of the same style, while distinct diagonal bands may suggest a stronger focus on semantic categorization.
    </div>
    <!-- image -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <figure class="image">
          <img src="./static/images/Model_Heatmaps.webp" alt="DINO-ViT-B-16_IN1k heatmaps indicating the mutual Mahalanobis distances of fruit-SALAD images." class="image">
        </figure>
      </div>
    </div>
    <!-- image -->
     <br><br>
         <!-- image -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <figure class="image">
          <img src="./static/images/Model_Heatmaps_Overview.webp" alt="Heatmaps indicating the mutual Mahalanobis distance of fruit-SALAD_10k images according to different models." class="image">
        </figure>
      </div>
    </div>
    <!-- image -->
  </div>
</div>
</section>
<!--/ Model Heatmaps. -->

<!-- Relative Model Comparison. -->
<section class="section">
  <div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Relative Model Comparison</h2>

    <div class="content has-text-justified">
      To compare models systematically, we treated their pairwise distance matrices as multidimensional vectors and embedded them in the same metric space. This enabled a relative comparison of the models, revealing patterns in how different architectures encode similarity.
    </div>
    <!-- image -->
    <div class="columns is-centered">
    <div class="column is-full-width">
      <figure class="image">
        <img src="./static/images/Model_Comparison.webp" alt="Relative model comparison using principal component analysis (PCA) based on 23 standardized model vectors of 4,950 dimensions." class="image">
      </figure>
    </div>
  </div>
  <!-- image -->
  </div>
</div>
</section>
<!--/ Relative Model Comparison. -->

<!-- fruit-SALAD Explorer. -->
<section class="section">
  <div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">fruit-SALAD Explorer</h2>

    <div class="content has-text-justified">
      Our <a href="https://style-aligned-artwork-datasets.github.io/fruit-explorer" target="_blank" rel="noopener noreferrer">interactive visualization tool</a> enables exploration of how different models perceive similarity. To achieve this, we used projection methods such as t-SNE, UMAP, and MDS to reduce the dimensionality of image embeddings, making the differences in similarity perception across models more interpretable.
    </div>
    <!-- image -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <figure class="image">
          <img src="./static/images/fruit-SALAD_Explorer_apples_oranges.webp" alt="Apples vs. Oranges comparison in the fruit-SALAD Explorer." class="image">
        </figure>
      </div>
    </div>
    <!-- image -->
  </div>
</div>
</section>
<!--/ fruit-SALAD Explorer. -->

<!-- Acknowledgements. -->

<section class="section">
  <div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Acknowledgements</h2>

    <div class="content has-text-justified">
      All authors were supported by the CUDAN ERA Chair project for Cultural Data Analytics, funded through the European Union’s
      Horizon 2020 research and innovation program (Grant No. 810961).
    </div>
  </div>
</div>
<!--/ Acknowledgements. -->
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{ohm2025FruitSALAD,
  title = {fruit-SALAD: A Style Aligned Artwork Dataset to reveal similarity perception in image embeddings},
  volume = {12},
  issn = {2052-4463},
  url = {https://doi.org/10.1038/s41597-025-04529-4},
  doi = {10.1038/s41597-025-04529-4},
  number = {1},
  journal = {Scientific Data},
  author = {Ohm, Tillmann and Karjus, Andres and Tamm, Mikhail V. and Schich, Maximilian},
  year = {2025},
  pages = {254},
}

@dataset{ohm_2024_11158522,
  author       = {Ohm, Tillmann},
  title        = {fruit-SALAD},
  month        = may,
  year         = 2024,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.11158522},
  url          = {https://doi.org/10.5281/zenodo.11158522},
}
    </code></pre>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop content">
    <div class="columns is-centered">
      <div class="column is-full-width">
<!-- Concurrent Work. -->

    <h2 class="title is-3">Related Work</h2>

    <div class="content has-text-justified">
      <p>
        The fruit-SALAD Explorer is based on our <a href="https://collection-space-navigator.github.io/" target="_blank" rel="noopener noreferrer">Collection Space Navigator.</a>
      </p>
    </div>
  </div>
</div>
<!--/ Concurrent Work. -->
</div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/CSN_vinci23_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/Style-Aligned-Artwork-Datasets/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website build using the <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
